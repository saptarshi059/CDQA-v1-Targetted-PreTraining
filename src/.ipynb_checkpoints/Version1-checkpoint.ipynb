{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f104b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Saptarshi7/covid_qa_cleaned_CS\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_contexts = set()\n",
    "for ctx in dataset['train']['context']:\n",
    "    all_contexts.add(ctx)\n",
    "\n",
    "sorted_all_contexts_on_len = sorted(list(all_contexts), key=len, reverse=True)    \n",
    "top_contexts = len(all_contexts)\n",
    "\n",
    "with open(\"covidqa-longest_len_context.txt\", \"w\") as text_file:\n",
    "    for idx, ctx in enumerate(sorted_all_contexts_on_len):\n",
    "        if idx == top_contexts:\n",
    "            break\n",
    "        text_file.write(ctx)\n",
    "        text_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1d760",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Generating the mapping table: (matched_text, CUI, preferred candidate)\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "texts = json.load(open('covidqa-context-cleaned4MM-MM_output.json'))['AllDocuments']\n",
    "\n",
    "Metamap_Tokenizations = []\n",
    "for doc in tqdm(texts):\n",
    "    for ctx_dict in doc['Document']['Utterances']:\n",
    "        mappings = []\n",
    "        ctx_text = ctx_dict['UttText']\n",
    "        ctx_start_idx = int(ctx_dict['UttStartPos'])\n",
    "        for phr in ctx_dict['Phrases']:\n",
    "            if phr['Mappings'] != []:\n",
    "                for phr_dict in phr[\"Mappings\"][0]['MappingCandidates']: #Choosing the first candidate\n",
    "                    start_idx = int(phr_dict['ConceptPIs'][0]['StartPos']) - ctx_start_idx\n",
    "                    end_idx = start_idx + int(phr_dict['ConceptPIs'][0]['Length'])\n",
    "                    mappings.append((ctx_text[start_idx:end_idx], phr_dict['CandidateCUI'], \\\n",
    "                                     phr_dict['CandidatePreferred']))\n",
    "        Metamap_Tokenizations.append((ctx_text, mappings))\n",
    "\n",
    "entities = set()\n",
    "for mappings in Metamap_Tokenizations:\n",
    "    for tup in mappings[1]:\n",
    "        entities.add(tup[2])\n",
    "print(f\"Number of entities discovered: {len(entities)}\")\n",
    "\n",
    "natural_text = [y[0] for x in Metamap_Tokenizations for y in x[1]]\n",
    "cuis = [y[1] for x in Metamap_Tokenizations for y in x[1]]\n",
    "pc = [y[2] for x in Metamap_Tokenizations for y in x[1]]\n",
    "CUI_Preferred_Concept_Lookup_Table = pd.DataFrame(zip(natural_text, cuis, pc), columns=['natural_text','CUI','Preferred_Concept']).drop_duplicates()\n",
    "CUI_Preferred_Concept_Lookup_Table.to_csv('natural_text_CUI_PC.csv', index=False)\n",
    "print('Our_CUI_PC table generated...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.read_csv('natural_text_CUI_PC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('csarron/bert-base-uncased-squad-v1')\n",
    "model_vocab = list(tokenizer.vocab.keys())\n",
    "pretrained_KGE_df = pd.read_csv('embeddings.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e37339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "for row in tqdm(s.itertuples(), total=s.shape[0]):\n",
    "    '''\n",
    "    If the entity is in the model_vocab remove that row since we don't want another embedding for the same\n",
    "    term. If the entities CUI is not in the list of pretrained ones, remove it since we can't learn a \n",
    "    mapping then.\n",
    "    '''\n",
    "    if (row.natural_text in model_vocab) or (row.CUI not in pretrained_KGE_df[0].to_list()):\n",
    "        s.drop(axis=0, index=row.Index, inplace=True)\n",
    "\n",
    "s.to_csv('Filtered_by_BERT_vocab_and_KGE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.read_csv('Filtered_by_BERT_vocab_and_KGE.csv')\n",
    "s.drop(axis=1, inplace=True, columns=['Unnamed: 0'])\n",
    "s.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2d7398",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 18:10:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f24062342ef4cc4b0011351627bdd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 18:10:09 INFO: Loading these models for language: en (English):\n",
      "==================================================================================================\n",
      "| Processor | Package                                                                            |\n",
      "--------------------------------------------------------------------------------------------------\n",
      "| tokenize  | combined                                                                           |\n",
      "| ner       | anatem;bc5cdr;bc4chemd;bionlp13cg;jnlpba;linnaeus;ncbi_disease;s800;i2b2;radiology |\n",
      "==================================================================================================\n",
      "\n",
      "2022-10-20 18:10:09 INFO: Use device: cpu\n",
      "2022-10-20 18:10:09 INFO: Loading: tokenize\n",
      "2022-10-20 18:10:09 INFO: Loading: ner\n",
      "2022-10-20 18:10:15 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline('en', package=None, processors={'ner':['anatem',\n",
    "                                                            'bc5cdr',\n",
    "                                                            'bc4chemd',\n",
    "                                                            'bionlp13cg',\n",
    "                                                            'jnlpba',\n",
    "                                                            'linnaeus',\n",
    "                                                            'ncbi_disease',\n",
    "                                                            's800',\n",
    "                                                            'i2b2',\n",
    "                                                            'radiology'], 'tokenize':'default'})\n",
    "                      #tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5414356",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28057/2820193503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ECMO extracorporeal membrane oxygenation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#if this is empty, it means, the NER didn't recognize anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbulk_process\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbulk\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretokenized'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no_ssplit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;34m\"If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object."
     ]
    }
   ],
   "source": [
    "doc = nlp([['ECMO extracorporeal membrane oxygenation']])\n",
    "print(doc.entities) #if this is empty, it means, the NER didn't recognize anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531eb18",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Don't need to run this now, since I run it on GPU01. It's extremely slow on cpu!\n",
    "from tqdm.notebook import tqdm\n",
    "for row in tqdm(s.itertuples(), total=s.shape[0]):\n",
    "    prepared_for_NER = [[str(row.natural_text)]]\n",
    "    #i.e. we want to keep only those entries that map to at least 1 entity\n",
    "    if nlp(prepared_for_NER).entities == []: \n",
    "        s.drop(axis=0, index=row.Index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.read_csv('Filtered_by_Stanza_NER.csv')\n",
    "s.drop(axis=1, inplace=True, columns=['Unnamed: 0'])\n",
    "s.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "texts = []\n",
    "split = []\n",
    "ents = []\n",
    "unique_entities = list(set(s.natural_text.to_list()))\n",
    "\n",
    "for ent in tqdm(unique_entities):\n",
    "    try:\n",
    "        texts.append(wikipedia.page(wikipedia.search(ent, results=1)).content)\n",
    "        split.append('train')\n",
    "        ents.append(str(ent))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "pd.DataFrame(zip(split, ents, texts), columns = ['split', 'ent', 'text']).to_csv('corpus.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
