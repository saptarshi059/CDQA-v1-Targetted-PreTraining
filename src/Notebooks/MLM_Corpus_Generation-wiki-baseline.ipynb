{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d21f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open('../../data/COVID-QA/top_N_ents_spacy-COVID_QA.pkl', 'rb') as f:\n",
    "    top_N_ents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "search_res = {}\n",
    "\n",
    "for i, ent in tqdm(enumerate(top_N_ents)):\n",
    "    #Skipping those entities which don't return anything\n",
    "    if wikipedia.search(ent) != []:\n",
    "        search_res[ent] = wikipedia.search(str(ent), results=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "context_dict = {}\n",
    "\n",
    "filtering = False\n",
    "if filtering == True:\n",
    "    filtering_threshold = 0.5\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    #We're consider scibert because pubmedbert assigns very high similarity for both related/unrelated terms.\n",
    "    checkpoint = 'allenai/scibert_scivocab_uncased'\n",
    "    \n",
    "    model = AutoModel.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    model.to(device)\n",
    "\n",
    "for ent,res in tqdm(search_res.items()):\n",
    "    if filtering == False:\n",
    "        try:\n",
    "            context_dict[ent] = wikipedia.page(res, auto_suggest=False).content          \n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        encoded_input = tokenizer([ent, res], return_tensors='pt', padding=True)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_input)\n",
    "        \n",
    "        similarity = cos(output.pooler_output[0], output.pooler_output[1])\n",
    "        '''\n",
    "        we're taking less than here since the similarity scores for related terms seem to be lower than \n",
    "        unrelated ones.\n",
    "        '''\n",
    "        if similarity.item() < filtering_thresold:\n",
    "            try:\n",
    "                context_dict[ent] = wikipedia.page(res, auto_suggest=False).content          \n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb95df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(context_dict.items(), columns = ['ent', 'text']).to_parquet('wiki_corpus.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "\n",
    "# Create a PubMed object that GraphQL can use to query\n",
    "# Note that the parameters are not required but kindly requested by PubMed Central\n",
    "# https://www.ncbi.nlm.nih.gov/pmc/tools/developers/\n",
    "pubmed = PubMed(tool=\"MyTool\", email=\"my@email.address\")\n",
    "\n",
    "# Create a GraphQL query in plain text\n",
    "query = ['covid-19', 'hiv-1']\n",
    "\n",
    "# Execute the query against the API\n",
    "results = pubmed.query(query, max_results=500)\n",
    "\n",
    "# Loop over the retrieved articles\n",
    "for article in results:\n",
    "\n",
    "    # Extract and format information from the article\n",
    "    article_id = article.pubmed_id\n",
    "    title = article.title\n",
    "    abstract = article.abstract\n",
    "\n",
    "    # Show information about the article\n",
    "    print(\n",
    "        f'{abstract}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4395f8ed",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "input_filename = '../../data/RadQA'\\\n",
    "                 '/radqa-a-question-answering-dataset-to-improve-comprehension-of-radiology-reports-1.0.0'\\\n",
    "                 '/dev.json'\n",
    "output_filename = '../../data/RadQA'\\\n",
    "                 '/radqa-a-question-answering-dataset-to-improve-comprehension-of-radiology-reports-1.0.0'\\\n",
    "                 '/dev.jsonl'\n",
    "\n",
    "with open(input_filename, encoding=\"utf-8\") as f:\n",
    "    radqa = json.load(f)\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in radqa[\"data\"]:\n",
    "        title = example.get(\"title\", \"\")\n",
    "        for paragraph in example[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]  # do not strip leading blank spaces GH-2585\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                id_ = qa[\"id\"]\n",
    "\n",
    "                answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                answers = [answer[\"text\"] for answer in qa[\"answers\"]]              \n",
    "                f.write(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"id\": id_,\n",
    "                    \"answers\": {\n",
    "                        \"answer_start\": answer_starts,\n",
    "                        \"text\": answers,\n",
    "                               },\n",
    "                        }\n",
    "                               )\n",
    "                       )\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "#ds = load_dataset(\"json\", data_files=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "07cf0053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7880e4e40e241c3a361915c9a8a7fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad_v2 (/home/saptarshi/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db7b12e7e4640219664dfcc4fbb3a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = load_dataset('squad_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5343a223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/saptarshi/.cache/huggingface/datasets/json/default-62a55fd6e6162c0e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2592d3b76f3a40338fbfd1f828fd4435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_raw = load_dataset('json', data_files='../../data/RadQA'\n",
    "                                                        '/radqa-a-question-answering-dataset-to-improve-comprehension'\n",
    "                                                        '-of-radiology-reports-1.0.0/train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f4b18e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '796653', 'context': 'IMPRESSION:  Subdural hematomas with blood products of different ages.\\n Question vescular abnormality in left suprasellar space.  Findings were\\n discussed with Dr. [**Last Name (STitle) 8620**] at 9:25 am on [**2191-8-5**].  An MRI of the brain and MRA\\n of the COW is recommended.', 'question': 'Is there any significant change in bleeding?', 'id': '796653_2_1_I', 'answers': {'answer_start': [], 'text': []}}\n"
     ]
    }
   ],
   "source": [
    "for row1 in train_dataset_raw['train']:\n",
    "    if row1['answers']['text'] == []:\n",
    "        print(row1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4a32560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5a8d7bf7df8bba001a0f9ab1', 'title': 'The_Legend_of_Zelda:_Twilight_Princess', 'context': 'The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b]', 'question': 'What category of game is Legend of Zelda: Australia Twilight?', 'answers': {'text': [], 'answer_start': []}}\n"
     ]
    }
   ],
   "source": [
    "for row in s['train']:\n",
    "    if row['answers']['text'] == []:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contexts = []\n",
    "all_q = []\n",
    "for row in s['train']['paragraphs']:\n",
    "    for d in row:\n",
    "        all_contexts.append(d[\"context\"])\n",
    "        for q in d['qas']:\n",
    "            all_q.append(q[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "c=0\n",
    "for r in all_contexts:\n",
    "    #if re.search('findings',r, re.IGNORECASE) and re.search('impression:', r, re.IGNORECASE):\n",
    "    #if re.search('final report', r, re.IGNORECASE) and re.search('impression:', r, re.IGNORECASE):\n",
    "    if re.search('findings and impression', r, re.IGNORECASE):\n",
    "        print(r)\n",
    "        print('-'*100)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
