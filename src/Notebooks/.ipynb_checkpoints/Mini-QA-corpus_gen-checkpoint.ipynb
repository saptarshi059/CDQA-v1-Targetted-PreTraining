{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff72e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "selected_stanza_ents = pd.read_csv('../../data/our-wikipedia-corpus/Tokens_From_Question_side/mini_corpus-10T5CpT.csv')\n",
    "stanza_ents_main = pd.read_pickle(open('../../data/stanza_ents-from_questions.pkl','rb'))\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_vocab = list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af10b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8d7cae85334c1293d5fa3ab7812364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#Removing those entities that are also in the model vocab since we don't want 2 of the same token embedding.\n",
    "ent_in_model_vocab = []\n",
    "for ent in tqdm(stanza_ents_main):\n",
    "    if ent in model_vocab:\n",
    "        ent_in_model_vocab.append(ent)\n",
    "\n",
    "for ent in ent_in_model_vocab:\n",
    "    stanza_ents_main.remove(ent)\n",
    "\n",
    "#Removing all occurrences of the selected entities. This is where we want fresh new entities & contexts.\n",
    "for ent in list(set(selected_stanza_ents.ent.to_list())):\n",
    "    if ent in stanza_ents_main:\n",
    "        stanza_ents_main = list(filter((ent).__ne__, stanza_ents_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddda626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "s = Counter(stanza_ents_main)\n",
    "sorted_ent_counts = dict(sorted(s.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f07cbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913a73d8859f4357bd98e6ea07022623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saptarshi/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/saptarshi/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities that were selected: dict_keys(['MERS-CoV', 'SARS', 'the study', 'MERS', 'swine', 'adenovirus', 'vaccination', 'SARS-CoV', 'the symptoms', 'hepcidin', 'swine flu', 'RSV', 'HMPV', 'zanamivir', 'the virus', 'total population infected', 'H1N1', 'respiratory viruses', 'hantaviruses', 'COVID', 'influenza viruses', 'DNA', '2019-nCoV', 'baculovirus', 'acute exacerbations', 'orf8', 'amino acid', 'bovine coronavirus', 'lower airway', 'What virus', 'the analysis', 'H5N1', 'the current study', 'most common', 'influenza infection', 'coronaviruses', 'covid', 'the delivery vector', 'host immune response', 'CEACAM1', 'AAV', 'chikungunya', '2019-nCoV virus', 'IFITM3', 'IFITM5', 'antiviral activity', 'NTCP', 'influenza virus', 'pertussis', 'b-cells'])\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from collections import defaultdict\n",
    "\n",
    "no_of_ents_to_select = 50\n",
    "selected_ents_text_dict = defaultdict(list)\n",
    "\n",
    "for ent in tqdm(sorted_ent_counts.keys()):\n",
    "    if len(selected_ents_text_dict.keys()) == no_of_ents_to_select:\n",
    "        break\n",
    "\n",
    "    search_result = wikipedia.search(str(ent), results=1)[0]\n",
    "    \n",
    "    try:\n",
    "        wiki_query = wikipedia.page(search_result, auto_suggest=False)\n",
    "        selected_ents_text_dict[ent].append(wiki_query.summary)\n",
    "        selected_ents_text_dict[ent].append(wiki_query.content)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Entities that were selected: {selected_ents_text_dict.keys()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e5f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = []\n",
    "title = []\n",
    "context = []\n",
    "question = []\n",
    "answers = []\n",
    "\n",
    "for idx, (ent, page_data) in enumerate(selected_ents_text_dict.items()):\n",
    "    sample_id.append(idx)\n",
    "    title.append(ent)\n",
    "    context.append(page_data[1])\n",
    "    question.append(f'What is {ent}')\n",
    "    answers.append({'text': [page_data[0]], 'answer_start': [page_data[1].find(page_data[0])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dd9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(sample_id, title, context, question, answers), \n",
    "             columns = ['id', 'title', 'context', 'question'\n",
    "                       ,'answers']).to_csv('mini-corpus-for-extended-QA.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
